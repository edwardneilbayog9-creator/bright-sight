{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "k0E-ZcQ7KXUV",
        "outputId": "65939cc5-5977-4398-d712-8828e17c8971"
      },
      "outputs": [],
      "source": [
        "!pip install timm torch torchvision matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36ti2hYsy_33",
        "outputId": "4a1d0125-db83-46a0-ac0d-c411375adf1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX_XpKDlMjQa",
        "outputId": "03e38fbe-6f45-4317-c984-1c237f29ed08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 4217 images in 4 classes.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. Unzip the file to the local Colab runtime for fast access\n",
        "zip_path = '/content/drive/MyDrive/eye_dataset.zip' # or your Drive path\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/data')\n",
        "\n",
        "# 2. Define your ViT Preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 3. Load the dataset using ImageFolder\n",
        "train_dataset = datasets.ImageFolder(root='/content/data/dataset', transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "print(f\"Loaded {len(train_dataset)} images in {len(train_dataset.classes)} classes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6GK-V-p0U6L",
        "outputId": "18708d4e-4ecf-4fec-eca4-bee93db1bb6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 3373\n",
            "Validation samples: 844\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Calculate the sizes for the split\n",
        "dataset_size = len(train_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "\n",
        "# Split the dataset randomly\n",
        "# We use a manual_seed for reproducibility, ensuring we get the same split every time\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size], generator=generator)\n",
        "\n",
        "print(f\"Training samples: {len(train_subset)}\")\n",
        "print(f\"Validation samples: {len(val_subset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JANcrgR0t0i",
        "outputId": "d3ef8f10-41f2-442f-ac71-e3c322bd7edd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch image shape: torch.Size([32, 3, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create DataLoaders\n",
        "# We shuffle the training data to prevent the model from learning the order of images\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Check the first batch\n",
        "images, labels = next(iter(train_loader))\n",
        "print(f\"Batch image shape: {images.shape}\") # Expecting [32, 3, 224, 224]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "78c1a2694fc5466f803619a1dbca0797",
            "4827c8bfa11d47c1962efebdcdf835b4",
            "6869eec6a43146f59961a8a58d7e0fea",
            "08875e108db4456982ba37414b24e619",
            "99a75cf59d4c481991321fef102af840",
            "136530537dbd4137a599ef551348edae",
            "4fa0af91979d4008bf6e638d580bf350",
            "58a9ee5fd88a4eeca9cd71dfebf7ccbd",
            "1bbfc68dcfc5459c90f0b6e9b1847fd2",
            "9d0adfcbf6344555b21118eb4bf495cd",
            "9bdf81d241974e8a82b2e7925cd7aef2"
          ]
        },
        "id": "ke-rE1Kr0uP-",
        "outputId": "281cd290-1bbe-453d-9e4f-597643fb2ea9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78c1a2694fc5466f803619a1dbca0797",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized and moved to cuda\n"
          ]
        }
      ],
      "source": [
        "import timm\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create the model\n",
        "# pretrained=True downloads the weights that help the model learn faster\n",
        "# num_classes=4 sets the final layer to match your 4 disease categories\n",
        "model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=4)\n",
        "\n",
        "# Move the model to the GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model initialized and moved to {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tEyWxQ51Aet"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer - using a small learning rate (1e-4) is safer for fine-tuning\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXNpiB9A1FwA",
        "outputId": "24bd8536-f36d-4e70-864e-cb31fc5127a4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import copy\n",
        "\n",
        "# 1. Training Parameters\n",
        "num_epochs = 40\n",
        "best_acc = 0.0\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "\n",
        "# Path to save the best model on your Google Drive\n",
        "best_model_path = '/content/drive/MyDrive/best_eye_vit_model.pth'\n",
        "\n",
        "print(f\"Starting Training for {num_epochs} epochs...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    # --- TRAINING PHASE ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad() # Zero the gradients\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1) # Get predictions\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward() # Backpropagation\n",
        "        optimizer.step() # Update weights\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_subset)\n",
        "    epoch_acc = running_corrects.double() / len(train_subset)\n",
        "\n",
        "    # --- VALIDATION PHASE ---\n",
        "    model.eval() # Set to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    val_corrects = 0\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient tracking\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    val_epoch_loss = val_loss / len(val_subset)\n",
        "    val_epoch_acc = val_corrects.double() / len(val_subset)\n",
        "\n",
        "    # Save metrics for plotting\n",
        "    train_losses.append(epoch_loss)\n",
        "    val_losses.append(val_epoch_loss)\n",
        "    train_accs.append(epoch_acc.item())\n",
        "    val_accs.append(val_epoch_acc.item())\n",
        "\n",
        "    # --- SAVE BEST MODEL ---\n",
        "    # Only save if this epoch's accuracy is higher than previous best\n",
        "    if val_epoch_acc > best_acc:\n",
        "        best_acc = val_epoch_acc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"--> Best Model Saved (Acc: {best_acc:.4f})\")\n",
        "\n",
        "    duration = time.time() - epoch_start\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "          f\"Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f} | \"\n",
        "          f\"Time: {duration:.0f}s\")\n",
        "\n",
        "print(f\"Training Complete! Highest Validation Accuracy: {best_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k9zVa50NKJH"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. ENHANCED Preprocessing (Training only)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5), # Standard for fundus medical images\n",
        "    transforms.RandomRotation(15), # Retinal images are rotationally invariant\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1), # Simulates camera variation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 2. VALIDATION Preprocessing (Standard)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 3. Create split-aware datasets\n",
        "base_path = '/content/data/dataset'\n",
        "full_train_ds = datasets.ImageFolder(root=base_path, transform=train_transform)\n",
        "full_val_ds = datasets.ImageFolder(root=base_path, transform=val_transform)\n",
        "\n",
        "# Ensure same indices are used as the baseline for a fair comparison\n",
        "indices = list(range(len(full_train_ds)))\n",
        "train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "\n",
        "train_loader = DataLoader(Subset(full_train_ds, train_idx), batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(Subset(full_val_ds, val_idx), batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIdl2ITbNNwJ",
        "outputId": "ba5224e0-ede3-4b93-a880-d164e3208bd5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "# Metrics storage for thesis plotting\n",
        "history = {\n",
        "    'train_loss': [], 'val_loss': [],\n",
        "    'train_acc': [], 'val_acc': []\n",
        "}\n",
        "\n",
        "print(\"Starting Training with Loss Tracking...\")\n",
        "\n",
        "for epoch in range(40):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    # --- 1. TRAINING PHASE ---\n",
        "    enhanced_model.train()\n",
        "    running_train_loss = 0.0\n",
        "    running_train_corrects = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = enhanced_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item() * images.size(0)\n",
        "        running_train_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    # Update Learning Rate\n",
        "    scheduler.step()\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # Calculate average train metrics\n",
        "    epoch_train_loss = running_train_loss / len(train_subset)\n",
        "    epoch_train_acc = running_train_corrects.double() / len(train_subset)\n",
        "\n",
        "    # --- 2. VALIDATION PHASE ---\n",
        "    enhanced_model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    running_val_corrects = 0\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculation for speed/memory\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = enhanced_model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            running_val_loss += loss.item() * images.size(0)\n",
        "            running_val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    # Calculate average validation metrics\n",
        "    epoch_val_loss = running_val_loss / len(val_subset)\n",
        "    epoch_val_acc = running_val_corrects.double() / len(val_subset)\n",
        "\n",
        "    # Save metrics to history\n",
        "    history['train_loss'].append(epoch_train_loss)\n",
        "    history['val_loss'].append(epoch_val_loss)\n",
        "    history['train_acc'].append(epoch_train_acc.item())\n",
        "    history['val_acc'].append(epoch_val_acc.item())\n",
        "\n",
        "    # --- 3. PRINT RESULTS ---\n",
        "    duration = time.time() - epoch_start\n",
        "    print(f\"Epoch {epoch+1}/40 | LR: {current_lr:.6f}\")\n",
        "    print(f\"Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f}\")\n",
        "    print(f\"Val Loss:   {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}\")\n",
        "    print(f\"Time: {duration:.0f}s | --------------------------------\")\n",
        "\n",
        "    # Save best model logic\n",
        "    if epoch_val_acc > best_acc:\n",
        "        best_acc = epoch_val_acc\n",
        "        torch.save(enhanced_model.state_dict(), '/content/drive/MyDrive/best_enhanced_vit.pth')\n",
        "        print(f\"--> Best Model Saved! (Val Acc: {best_acc:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRUmy4GHNRYE",
        "outputId": "109a5144-609e-4283-d627-669eba8c9ad2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Re-run your unzipping and DataLoader setup here\n",
        "# (Make sure 'val_loader' is ready)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBjUMClm1MRY",
        "outputId": "35ed9236-b079-462e-df01-6b2367f4366c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Dataset extracted successfully.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Unzip the dataset to the local runtime for speed\n",
        "zip_path = '/content/drive/MyDrive/eye_dataset.zip'\n",
        "extract_path = '/content/data'\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"‚úÖ Dataset extracted successfully.\")\n",
        "else:\n",
        "    print(\"‚úÖ Dataset already extracted.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQErvz2e1yVT",
        "outputId": "07b8a7bc-469e-4b7e-926f-10739c22eeac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ val_loader ready with 844 images.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Standard normalization for ViT\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# The folder structure is usually /content/data/dataset/Normal, etc.\n",
        "# Check your path if this throws an error.\n",
        "data_path = '/content/data/dataset'\n",
        "full_dataset = datasets.ImageFolder(root=data_path, transform=transform)\n",
        "\n",
        "# Re-split to get the 20% validation set\n",
        "indices = list(range(len(full_dataset)))\n",
        "_, val_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "\n",
        "val_subset = Subset(full_dataset, val_idx)\n",
        "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define classes for the report\n",
        "class_names = full_dataset.classes # ['Cataract', 'Diabetic_Retinopathy', 'Glaucoma', 'Normal']\n",
        "print(f\"‚úÖ val_loader ready with {len(val_subset)} images.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "F4iIyJ25507I",
        "outputId": "76748690-6ff7-4f45-aa75-943a45caa206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Evaluating V1...\n",
            "üîç Evaluating V2...\n",
            "üîç Evaluating V3...\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import timm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "model_paths = {\n",
        "    \"V1\": \"/content/drive/MyDrive/best_eye_vit_model.pth\",\n",
        "    \"V2\": \"/content/drive/MyDrive/enhanced_eye_vit.pth\",\n",
        "    \"V3\": \"/content/drive/MyDrive/best_enhanced_vit.pth\"\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, path in model_paths.items():\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"‚ö†Ô∏è Skipping {name}: File not found.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"üîç Evaluating {name}...\")\n",
        "    model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=4).to(device)\n",
        "\n",
        "    # Load weights safely\n",
        "    model.load_state_dict(torch.load(path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    val_loss = 0.0\n",
        "    val_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    results[name] = {\n",
        "        \"loss\": val_loss / len(val_subset),\n",
        "        \"acc\": val_corrects.double().item() / len(val_subset)\n",
        "    }\n",
        "\n",
        "# Plot the results\n",
        "if results:\n",
        "    names = list(results.keys())\n",
        "    accs = [results[n]['acc'] for n in names]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(names, accs, color=['skyblue', 'lightgreen', 'salmon'])\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Final Comparison of Model Versions')\n",
        "    for i, v in enumerate(accs):\n",
        "        plt.text(i, v + 0.01, f\"{v:.2%}\", ha='center')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNdxV7zf57L8",
        "outputId": "e842b543-1e77-45fb-9e95-7ca3da910461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Generating Confusion Matrix for V1...\n",
            "\n",
            "--- V1 Classification Report ---\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "            cataract       0.98      0.99      0.99       233\n",
            "diabetic_retinopathy       1.00      1.00      1.00       224\n",
            "            glaucoma       0.99      0.97      0.98       188\n",
            "              normal       0.99      0.99      0.99       199\n",
            "\n",
            "            accuracy                           0.99       844\n",
            "           macro avg       0.99      0.99      0.99       844\n",
            "        weighted avg       0.99      0.99      0.99       844\n",
            "\n",
            "üìä Generating Confusion Matrix for V2...\n",
            "\n",
            "--- V2 Classification Report ---\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "            cataract       0.45      0.72      0.55       233\n",
            "diabetic_retinopathy       0.36      0.72      0.48       224\n",
            "            glaucoma       0.20      0.01      0.02       188\n",
            "              normal       0.21      0.02      0.03       199\n",
            "\n",
            "            accuracy                           0.40       844\n",
            "           macro avg       0.31      0.37      0.27       844\n",
            "        weighted avg       0.32      0.40      0.29       844\n",
            "\n",
            "üìä Generating Confusion Matrix for V3...\n",
            "\n",
            "--- V3 Classification Report ---\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "            cataract       0.90      0.93      0.92       233\n",
            "diabetic_retinopathy       1.00      0.99      0.99       224\n",
            "            glaucoma       0.82      0.82      0.82       188\n",
            "              normal       0.84      0.81      0.83       199\n",
            "\n",
            "            accuracy                           0.89       844\n",
            "           macro avg       0.89      0.89      0.89       844\n",
            "        weighted avg       0.89      0.89      0.89       844\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import timm\n",
        "import os\n",
        "\n",
        "# 1. Setup the plot area for 3 confusion matrices\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "# Use full_dataset since that is what we defined in the loading step\n",
        "classes = full_dataset.classes\n",
        "\n",
        "for i, (name, path) in enumerate(model_paths.items()):\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Skipping {name}: path not found.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"üìä Generating Confusion Matrix for {name}...\")\n",
        "\n",
        "    # Initialize and Load model\n",
        "    model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=4).to(device)\n",
        "    model.load_state_dict(torch.load(path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    # Collect predictions from the val_loader\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    # 2. Compute Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # 3. Plot Heatmap on the specific subplot\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
        "                xticklabels=classes, yticklabels=classes, cbar=False)\n",
        "    axes[i].set_title(f\"Confusion Matrix: {name}\")\n",
        "    axes[i].set_ylabel('Actual')\n",
        "    axes[i].set_xlabel('Predicted')\n",
        "\n",
        "    # Print text report for detailed thesis metrics\n",
        "    print(f\"\\n--- {name} Classification Report ---\")\n",
        "    print(classification_report(y_true, y_pred, target_names=classes))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfumnNqWJwSu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08875e108db4456982ba37414b24e619": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d0adfcbf6344555b21118eb4bf495cd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9bdf81d241974e8a82b2e7925cd7aef2",
            "value": "‚Äá346M/346M‚Äá[00:03&lt;00:00,‚Äá208MB/s]"
          }
        },
        "136530537dbd4137a599ef551348edae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bbfc68dcfc5459c90f0b6e9b1847fd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4827c8bfa11d47c1962efebdcdf835b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_136530537dbd4137a599ef551348edae",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4fa0af91979d4008bf6e638d580bf350",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "4fa0af91979d4008bf6e638d580bf350": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58a9ee5fd88a4eeca9cd71dfebf7ccbd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6869eec6a43146f59961a8a58d7e0fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58a9ee5fd88a4eeca9cd71dfebf7ccbd",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1bbfc68dcfc5459c90f0b6e9b1847fd2",
            "value": 346284714
          }
        },
        "78c1a2694fc5466f803619a1dbca0797": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4827c8bfa11d47c1962efebdcdf835b4",
              "IPY_MODEL_6869eec6a43146f59961a8a58d7e0fea",
              "IPY_MODEL_08875e108db4456982ba37414b24e619"
            ],
            "layout": "IPY_MODEL_99a75cf59d4c481991321fef102af840"
          }
        },
        "99a75cf59d4c481991321fef102af840": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bdf81d241974e8a82b2e7925cd7aef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d0adfcbf6344555b21118eb4bf495cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
